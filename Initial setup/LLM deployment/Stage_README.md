# LLASTA ‚Äì Stage_README (ECR Pull-Through + EBS Snapshot + vLLM, ClusterIP/port-forward)
R√©gion: **us-east-1** ¬∑ Compte: **142473567252** ¬∑ Mod√®le: **Qwen3-8B** ¬∑ Runtime: **vLLM (OpenAI-compatible)**

Ce document couvre :
- **Initialisation (one-time)** : config ECR **Pull-Through Cache** (PTC) et "priming" des poids sur EBS.
- **D√©ploiement** : d√©ploiement du **runtime vLLM** avec les poids, acc√®s via **ClusterIP + port-forward**, tests.

> **üí° Note sur les snapshots** : Pour l'apprentissage avec vLLM en lecture seule, les snapshots ne sont **pas n√©cessaires**. Le PVC persistant suffit ! Les snapshots sont utiles pour la production multi-environnements.

> **Pr√©-requis**
> - `aws` CLI, `kubectl`, `jq` install√©s.
> - Cluster **EKS** (auth IAM ok) avec **n≈ìuds GPU** (AMI NVIDIA) + **NVIDIA device plugin**.
> - **AWS EBS CSI driver** install√©.
> - Les fichiers fournis dans ce dossier :  
>   `00-namespace.yaml` ¬∑ `01-storageclasses.yaml` ¬∑ `02-pvc-source.yaml` ¬∑ `03-job-prime-weights.yaml` ¬∑ `11-deploy-vllm.yaml`

---

## 0) Acc√®s au cluster (IAM ‚Üí kubeconfig)

```bash
aws eks update-kubeconfig --region us-east-1 --name llasta
kubectl get nodes
```

---

## 1) INITIALISATION (one-time)

### 1.0 V√©rifier et installer les composants EKS n√©cessaires

**V√©rifier l'EBS CSI Driver** (requis pour les volumes EBS) :
```bash
aws eks describe-addon --cluster-name llasta --addon-name aws-ebs-csi-driver --region us-east-1
```

Si pas install√© :
```bash
aws eks create-addon --cluster-name llasta --addon-name aws-ebs-csi-driver --region us-east-1
```

**V√©rifier que les pods EBS CSI fonctionnent** :
```bash
kubectl get pods -n kube-system | grep ebs
# Doit afficher des pods ebs-csi-controller et ebs-csi-node en Running
```

**IMPORTANT : Ajouter les permissions EBS au r√¥le des n≈ìuds** :
```bash
# Cette √©tape est CRUCIALE pour que l'EBS CSI Driver puisse cr√©er des volumes
aws iam attach-role-policy --role-name eks-node-role --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
```

**V√©rifier que les permissions sont appliqu√©es** :
```bash
aws iam list-attached-role-policies --role-name eks-node-role
# Doit inclure AmazonEBSCSIDriverPolicy dans la liste
```

**Installer le NVIDIA Device Plugin** (requis pour exposer les GPU aux pods) :
```bash
# Installer le NVIDIA Device Plugin
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml

# V√©rifier que les pods NVIDIA d√©marrent
kubectl get pods -n kube-system -l name=nvidia-device-plugin-ds

# Attendre la d√©tection des GPU (30-60 secondes)
sleep 60

# V√©rifier que les GPU sont maintenant visibles dans Kubernetes
kubectl describe nodes | grep -A 5 -B 5 nvidia.com/gpu
# Doit afficher: nvidia.com/gpu: 1 dans Capacity et Allocatable
```

> **üí° Pourquoi cette √©tape ?** L'AMI `AL2_x86_64_GPU` contient les drivers NVIDIA, mais le **Device Plugin** est n√©cessaire pour exposer les ressources GPU √† l'API Kubernetes. Sans lui, les pods ne peuvent pas demander de ressources `nvidia.com/gpu`.



### 1.1 Configurer ECR Pull-Through Cache (PTC) pour `vllm/vllm-openai`

1) Secret Docker Hub (√©vite rate-limits) :
```bash
aws secretsmanager create-secret   --name ecr-pullthroughcache/dockerhub   --secret-string '{"username":"<DOCKERHUB_USER>","accessToken":"<DOCKERHUB_TOKEN>"}'   --region us-east-1
```

2) R√®gle PTC :
```bash
aws ecr create-pull-through-cache-rule   --ecr-repository-prefix dockerhub   --upstream-registry-url registry-1.docker.io   --credential-arn arn:aws:secretsmanager:us-east-1:142473567252:secret:ecr-pullthroughcache/dockerhub   --region us-east-1
```

3) **R√©f√©rence d‚Äôimage** √† utiliser c√¥t√© K8s :  
`142473567252.dkr.ecr.us-east-1.amazonaws.com/dockerhub/vllm/vllm-openai:<tag>`  
(Premier pull depuis l‚Äôamont; suivants depuis ECR local.)

> **Note IAM n≈ìuds** : attache au r√¥le des n≈ìuds la policy `AmazonEC2ContainerRegistryReadOnly` pour autoriser les pulls depuis ECR priv√©.

---

### 1.2 "Primer" un volume EBS avec les poids Qwen3-8B

> Objectif : t√©l√©charger une fois les poids du mod√®le sur un volume persistant pour r√©utilisation directe par vLLM.

1) **Cr√©er le namespace** + classes de stockage/snapshot
```bash
kubectl apply -f 00-namespace.yaml
kubectl config set-context llasta --namespace=llasta
kubectl apply -f 01-storageclasses.yaml
```

**Note importante** : Maintenant que les CRDs sont install√©s, les `StorageClass` ET `VolumeSnapshotClass` devraient √™tre cr√©√©es sans erreur.

**V√©rifier que la StorageClass est cr√©√©e** :
```bash
kubectl get storageclass
# Doit afficher 'gp3' avec provisioner 'ebs.csi.aws.com'
```

2) **PVC source** (re√ßoit les poids)
```bash
kubectl apply -f 02-pvc-source.yaml
kubectl get pvc qwen3-weights-src
```

**√âtat attendu** : `STATUS=Pending` avec message `WaitForFirstConsumer`. C'est **normal** ! Le volume EBS sera cr√©√© quand un pod utilisera le PVC.

**Si le PVC reste en erreur** (ex: `storageclass.storage.k8s.io "gp3" not found`), recr√©ez-le :
```bash
kubectl delete -f 02-pvc-source.yaml
kubectl apply -f 02-pvc-source.yaml
```

3) **Job de priming** (t√©l√©charge `Qwen/Qwen3-8B` ‚Üí PVC)
```bash
kubectl create secret generic hf-token --from-literal=token=<HF_Token> -n llasta
```

> **üìù Note** : Le token Hugging Face n'est **pas n√©cessaire** pour Qwen3-8B car ce mod√®le est **public** (licence Apache 2.0). Le secret `hf-token` est configur√© pour compatibilit√© avec d'autres mod√®les priv√©s.

```bash
kubectl apply -f 03-job-prime-weights.yaml
```

**Surveiller le progr√®s** :
```bash
# Voir l'√©tat du job
kubectl get jobs -w

# Voir les logs en temps r√©el
kubectl logs -f job/prime-qwen3-8b

# V√©rifier que le PVC est maintenant Bound
kubectl get pvc qwen3-weights-src
```

**Attendre la completion** :
```bash
kubectl -n llasta wait --for=condition=complete job/prime-qwen3-8b --timeout=2h
```

**V√©rifier le contenu t√©l√©charg√©** (optionnel) :
```bash
# Cr√©er un pod debug pour explorer le volume
kubectl apply -f debug-pod.yaml

# Se connecter au pod et explorer
kubectl exec -it debug-volume -n llasta -- sh
# Dans le pod : ls -la /models/Qwen3-8B/
# Dans le pod : du -sh /models/Qwen3-8B/

# Nettoyer le pod debug
kubectl delete pod debug-volume -n llasta
```

4) **Prot√©ger et tagger le volume EBS**

Section √† supprimer

5) **V√©rification finale**
```bash
# V√©rifier que le PVC est bien Bound avec les poids
kubectl get pvc qwen3-weights-src -n llasta

# Optionnel : nettoyer le job (garder le PVC pour vLLM)
kubectl delete job prime-qwen3-8b -n llasta
```

> **üéâ F√©licitations !** Vos poids Qwen3-8B sont maintenant disponibles sur le volume persistant `qwen3-weights-src`, **prot√©g√©s contre la suppression** et **tagu√©s pour r√©cup√©ration facile**. Vous pouvez passer directement au d√©ploiement vLLM !

---

## 1.3) TROUBLESHOOTING - Probl√®mes courants

### PVC reste en `Pending` avec erreur `storageclass not found`
```bash
# V√©rifier que la StorageClass existe
kubectl get storageclass gp3

# Si elle n'existe pas, la recr√©er
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
  fsType: ext4
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF

# Puis recr√©er le PVC
kubectl delete -f 02-pvc-source.yaml
kubectl apply -f 02-pvc-source.yaml
```

### EBS CSI Driver non install√©
```bash
# V√©rifier l'addon
aws eks describe-addon --cluster-name llasta --addon-name aws-ebs-csi-driver --region us-east-1

# Installer si n√©cessaire
aws eks create-addon --cluster-name llasta --addon-name aws-ebs-csi-driver --region us-east-1
```

### Erreur de permissions "UnauthorizedOperation: ec2:CreateVolume"
Si vous obtenez cette erreur lors de la cr√©ation de PVC :
```bash
# Ajouter les permissions EBS CSI Driver au r√¥le des n≈ìuds
aws iam attach-role-policy --role-name eks-node-role --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy

# Attendre 1-2 minutes pour la propagation des permissions
# Puis v√©rifier que le PVC passe √† "Bound"
kubectl get pvc -n llasta
```

---

## 2) D√âPLOIEMENT vLLM

### 2.1 R√©cup√©rer un volume EBS existant (si cluster recr√©√©)

> **Cas d'usage** : Vous avez supprim√© votre cluster K8s mais vos poids Qwen3-8B sont toujours dans un volume EBS gr√¢ce √† `reclaimPolicy: Retain`.

**√âtape 1 : Identifier le volume EBS avec vos poids**
```bash
# Lister tous les volumes EBS avec des tags du projet
aws ec2 describe-volumes \
  --filters "Name=tag:Project,Values=llasta" \
  --query 'Volumes[*].{VolumeId:VolumeId,Size:Size,State:State,Tags:Tags}' \
  --output table

# Ou chercher par nom si vous avez tagu√© vos volumes
aws ec2 describe-volumes \
  --filters "Name=tag:Name,Values=*qwen3*" \
  --query 'Volumes[*].{VolumeId:VolumeId,Size:Size,State:State,CreateTime:CreateTime}' \
  --output table
```

**√âtape 2 : V√©rifier que le volume est dans la bonne AZ**
```bash
# Obtenir l'AZ de vos n≈ìuds K8s
NODE_AZ=$(kubectl get nodes -o jsonpath='{.items[0].metadata.labels.topology\.kubernetes\.io/zone}')
echo "N≈ìuds K8s dans l'AZ: $NODE_AZ"

# V√©rifier l'AZ du volume (doit correspondre)
VOLUME_ID="vol-xxxxxxxxxxxxxxxxx"  # Remplacer par votre volume ID
VOLUME_AZ=$(aws ec2 describe-volumes --volume-ids $VOLUME_ID --query 'Volumes[0].AvailabilityZone' --output text)
echo "Volume dans l'AZ: $VOLUME_AZ"

# Si les AZ ne correspondent pas, cr√©er un snapshot et un nouveau volume dans la bonne AZ
if [ "$NODE_AZ" != "$VOLUME_AZ" ]; then
  echo "‚ö†Ô∏è  AZ diff√©rentes ! Cr√©ation d'un snapshot et nouveau volume n√©cessaire..."
  SNAPSHOT_ID=$(aws ec2 create-snapshot --volume-id $VOLUME_ID --description "Qwen3-8B migration" --query 'SnapshotId' --output text)
  aws ec2 wait snapshot-completed --snapshot-ids $SNAPSHOT_ID
  VOLUME_ID=$(aws ec2 create-volume --snapshot-id $SNAPSHOT_ID --volume-type gp3 --availability-zone $NODE_AZ --query 'VolumeId' --output text)
  aws ec2 wait volume-available --volume-ids $VOLUME_ID
  echo "‚úÖ Nouveau volume cr√©√©: $VOLUME_ID"
fi
```

**√âtape 3 : Cr√©er un PV/PVC pointant vers ce volume**
```bash
# Cr√©er le PersistentVolume qui r√©f√©rence votre volume EBS existant
kubectl apply -f 10-pvc-from-ebs.yaml
```

**√âtape 4 : V√©rifier que le PVC est bien li√©**
```bash
kubectl get pvc qwen3-weights-src
# Statut attendu: Bound

**V√©rifier le contenu t√©l√©charg√©** (optionnel) :
```bash
# Cr√©er un pod debug pour explorer le volume
kubectl apply -f debug-pod.yaml

# Se connecter au pod et explorer
kubectl exec -it debug-volume -n llasta -- sh
# Dans le pod : ls -la /models/Qwen3-8B/
# Dans le pod : du -sh /models/Qwen3-8B/

# Nettoyer le pod debug
kubectl delete pod debug-volume -n llasta
```

### 2.2 D√©ployer le runtime **vLLM** (image via ECR PTC)
```bash
kubectl apply -f 11-deploy-vllm.yaml
kubectl -n llasta rollout status deploy/vllm-qwen3
kubectl -n llasta get pods -l app=vllm-qwen3 -w
```
Si erreur pour le t√©l√©chargement depuis ECR, voir si la cr√©ation d'un ECR en AWS CLI r√©soud le probl√®me.

> `11-deploy-vllm.yaml` utilise un Service **ClusterIP**. L‚Äôimage attend les flags (entrypoint de l‚ÄôAPI vLLM).  
> Apr√®s le premier test, **pense √† pinner un tag** (√©vite `latest`).

### 2.3 Acc√©der via **port-forward**
```bash
kubectl -n llasta port-forward svc/vllm-svc 8000:8000
```
API locale : `http://127.0.0.1:8000`

---

## 3) TESTS

### 3.1 `curl` ‚Äì Chat Completions
```bash
curl -s "http://127.0.0.1:8000/v1/chat/completions"   -H "Content-Type: application/json"   -H "Authorization: Bearer sk-fake"   -d '{
    "model": "Qwen/Qwen3-8B",
    "messages": [{"role":"user","content":"Bonjour Qwen3, r√©sume LLASTA en une phrase."}]
  }' | jq .
```

### 3.2 Python (client OpenAI)
```python
from openai import OpenAI
client = OpenAI(base_url="http://127.0.0.1:8000/v1", api_key="sk-fake")
resp = client.chat.completions.create(
    model="Qwen/Qwen3-8B",
    messages=[{"role":"user","content":"Donne une punchline sur LLASTA."}],
)
print(resp.choices[0].message.content)
```

---

## 4) CLEAN-UP (quotidien, snapshot conserv√©)
```bash
kubectl -n llasta delete deploy vllm-qwen3 svc vllm-svc pvc qwen3-weights
```

---

## 5) D√âPANNAGE & COH√âRENCE

- **Ordre des fichiers corrig√©** : `00-namespace.yaml` est **nouveau** pour garantir que `llasta` existe avant tout objet namespac√©.
- **vLLM args corrig√©s** : l‚Äôimage `vllm/vllm-openai` a un entrypoint API; on passe `--model /models/Qwen3-8B --host 0.0.0.0 --port 8000 ...` (au lieu de `vllm serve`).
- **ReadinessProbe** ajust√©e (d√©marrage long possible) : `initialDelaySeconds: 60`, `failureThreshold: 60`.
- **PVC/Snapshot** : noms/namespace **align√©s** (`qwen3-weights-snap` en `llasta`). Restauration: `10-pvc-from-snapshot.yaml`.
- **ECR PTC** : URL d‚Äôimage **avec pr√©fixe** `dockerhub/`; r√¥les n≈ìuds avec `AmazonEC2ContainerRegistryReadOnly`.
- **Taints GPU** : le Deployment tol√®re `nvidia.com/gpu: NoSchedule`. Si tes n≈ìuds ne sont pas taint√©s, tu peux supprimer la section `tolerations`.
- **S√©curit√©** : Service **ClusterIP**, acc√®s via `kubectl port-forward`, pas d‚Äôexposition publique.

---

## 6) R√âSUM√â COMMANDES

### Initialisation
```bash
aws eks update-kubeconfig --region us-east-1 --name <CLUSTER>

# ECR Pull-Through Cache
aws secretsmanager create-secret --name ecr-pullthroughcache/dockerhub   --secret-string '{"username":"<DOCKERHUB_USER>","accessToken":"<DOCKERHUB_TOKEN>"}' --region us-east-1
aws ecr create-pull-through-cache-rule --ecr-repository-prefix dockerhub   --upstream-registry-url registry-1.docker.io   --credential-arn arn:aws:secretsmanager:us-east-1:142473567252:secret:ecr-pullthroughcache/dockerhub --region us-east-1

# EBS + Snapshot
kubectl apply -f 00-namespace.yaml
kubectl apply -f 01-storageclasses.yaml
kubectl apply -f 02-pvc-source.yaml
kubectl apply -f 03-job-prime-weights.yaml
kubectl -n llasta wait --for=condition=complete job/prime-qwen3-8b --timeout=2h
kubectl apply -f 04-snapshot.yaml
kubectl -n llasta get volumesnapshot qwen3-weights-snap -o jsonpath='{.status.readyToUse}'; echo
# optionnel
kubectl -n llasta delete job prime-qwen3-8b
kubectl -n llasta delete pvc qwen3-weights-src
```

### Quotidien
```bash
kubectl apply -f 10-pvc-from-snapshot.yaml
kubectl apply -f 11-deploy-vllm.yaml
kubectl -n llasta rollout status deploy/vllm-qwen3
kubectl -n llasta port-forward svc/vllm-svc 8000:8000

# Tests
curl -s "http://127.0.0.1:8000/v1/chat/completions"   -H "Content-Type: application/json" -H "Authorization: Bearer sk-fake"   -d '{"model":"Qwen/Qwen3-8B","messages":[{"role":"user","content":"Bonjour Qwen3 !"}]}'

# Clean-up
kubectl -n llasta delete deploy vllm-qwen3 svc vllm-svc pvc qwen3-weights
```
